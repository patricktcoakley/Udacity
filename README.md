# Udacity

This repo is home to my [Udacity](https://www.udacity.com) projects. Udacity is a site for self-learning new technologies, and their nanodegree programs are tracks that combine courses with projects that are graded by Udacity staff for the relevant field. I completed the [Data Analyst Nanodegree Program](https://www.udacity.com/course/data-analyst-nanodegree--nd002) as of June 2017 as a part of my Bachelor's program. The program may have slight differences between course selection and projects in the future. 

Some of the projects have been tweaked a bit since completion, including trying to format code for 80-column where possible. Other than that, they are presented as close to how they were at the time of submission as possible. Working with so many different technologies in various projects has made me appreciate the idea of following coding standards, and going forward I will try to adhere to them where applicable.

Here is a breakdown of each project I completed, including the learning objectives:

## Statistics
This course was an introductory course on statistics, and how to we can apply it to data science and analysis. The final project was an analysis of the [Stroop effect](https://en.wikipedia.org/wiki/Stroop_effect) using what we learned in class, including:

- How to identify components of an experiment
- How to use descriptive statistics to describe qualities of a sample
- How to set up a hypothesis test, make inferences from a sample, and draw conclusions based on the results


## Intro to Data Analysis
This is a follow-up to the previous course, this time with an emphasis on analyzing a dataset and reporting the findings. This included using Python 2.7 and various libraries, such as [NumPy](http://www.numpy.org) and [matplotlib](http://matplotlib.org). The final project was based around the widely used Titanic dataset, found [here](http://www.kaggle.com/c/titanic). The objectives for the project were:

- Know all the steps involved in a typical data analysis process
- Be comfortable posing questions that can be answered with a given dataset and then answering those questions
- Know how to investigate problems in a dataset and wrangle the data into a format you can use
- Have practice communicating the results of your analysis
- Be able to use vectorized operations in NumPy and Pandas to speed up your data analysis code
- Be familiar with Pandas' Series and DataFrame objects, which let you access your data more conveniently
- Know how to use Matplotlib to produce plots showing your findings

## Data Wrangling
Data Wrangling is a course moving and changing data around into different forms. The final project was based on taking data from the [OpenStreetMap](http://www.openstreetmap.org) project and putting it into a database: either [MongoDB](https://www.mongodb.com) or [SQLite](https://www.sqlite.org). Some of the tasks required of the project were:

- Assess the quality of the data for validity, accuracy, completeness, consistency and uniformity.
- Parse and gather data from popular file formats such as .csv, .json, .xml, and .html
- Process data from multiple files or very large files that can be cleaned programmatically.
- Learn how to store, query, and aggregate data using MongoDB or SQL.

## Exploratory Data Analysis
This course is based around [the R programming language](https://www.r-project.org) and how to apply exploratory data analysis techniques to explore the different relationships found in datasets. These techniques included:

- Understand the distribution of a variable and to check for anomalies and outliers
- Learn how to quantify and visualize individual variables within a data set by using appropriate plots such as scatter plots, histograms, bar charts, and box plots
- Explore variables to identify the most important variables and relationships within a data set before building predictive models; calculate correlations, and investigate conditional means
- Learn powerful methods and visualizations for examining relationships among multiple variables, such as reshaping data frames and using aesthetics like color and shape to uncover more information

## Intro to Machine Learning
Intro to Machine Learning explores the world of ML using Python and various popular libraries, including [scikit-learn](http://scikit-learn.org/stable/). The end result was creating a model that would be able to identify a person of interest inside the Enron email dataset. Objectives included:

- Deal with an imperfect, real-world dataset
- Validate a machine learning result using test data
- Evaluate a machine learning result using quantitative metrics
- Create, select and transform features
- Compare the performance of machine learning algorithms
- Tune machine learning algorithms for maximum performance
- Communicate your machine learning algorithm results clearly

## Data Visualization
Data Visualization is an introductory course to visualizing data using different tools, including [Tableau](https://www.tableau.com) and JavaScript libraries, such as [d3.js](https://d3js.org). The final project is to tell a story using a pre-existing or custom dataset in a way that is explanatory. The final project's objectives were:

- Demonstrate the ability to choose optimal visual elements to encode data and critically assess the effectiveness of the visualization
- Communicate a story or finding to the appropriate audience using interactive visualizations
- Undergo the iterative process of creating a visualization, and build interactive visualizations with dimple.js or d3.js.

## A/B Testing
This course goes over the concepts of A/B testing from start to finish, including the design and implementation. From there the final project is create one using simulated data to determine the efficacy of potential improvements to a product while specifying metrics to measure. This included the following:

- Select metrics to evaluate a proposed change
- Characterize and validate those metrics
- Plan an appropriate duration for your experiment based on the number of samples needed and the expected risk
- Sanity check the results to make sure everything went smoothly
- Draw a conclusion based on the results
- Recommend whether or not to launch the change